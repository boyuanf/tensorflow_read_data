{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Parse parameter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, the FLAGS is not tf.app.flags.FLAGS, it just a globel variable, which will not pass to main() as paramter, but it will be directly used by functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(learning_rate=0.03, num_epochs=2)\n0.03\n['--foo', '--badger', 'BAR', 'spam']\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      type=float,\n",
    "      default=0.01,\n",
    "      help='Initial learning rate.'\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "      '--num_epochs',\n",
    "      type=int,\n",
    "      default=2,\n",
    "      help='Number of epochs to run trainer.'\n",
    ")\n",
    "\n",
    "FLAGS, unparsed = parser.parse_known_args(['--learning_rate', '0.03', '--foo', '--badger', 'BAR', 'spam'])\n",
    "\n",
    "print(FLAGS)\n",
    "print(FLAGS.learning_rate)\n",
    "print(unparsed)\n",
    "\n",
    "tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.app.flags.FLAGS is used to pass parameters through the argument of main(), the value of FLAGS.max_steps and FLAGS.num_gpus become 50 and 20 (initialized as 100 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n20\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ],
     "output_type": "error"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Boyuan.Feng\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_integer('max_steps', 100,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_gpus', 1,\n",
    "                            \"\"\"How many GPUs to use.\"\"\")\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    print(FLAGS.max_steps)\n",
    "    print(FLAGS.num_gpus)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # the first param for argv is the program name\n",
    "  tf.app.run(main=main, argv=['tensorflow_read_data', '--max_steps', '50', '--num_gpus', '20'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 7 3 4] 5\n[66 77 33 44] 55\n[666 777 333 444] 555\n[1 2 3 4] 5\n[11 22 33 44] 55\n[111 222 333 444] 555\n[6 7 3 4] 5\n[66 77 33 44] 55\n[666 777 333 444] 555\n[1 2 3 4] 5\n[11 22 33 44] 55\n[111 222 333 444] 555\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "filename_queue = tf.train.string_input_producer([\".\\\\file0.csv\",\n",
    "                                                 \".\\\\file1.csv\"],\n",
    "                                                num_epochs=2)\n",
    "\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "# Default values, in case of empty columns. Also specifies the type of the\n",
    "# decoded result.\n",
    "record_defaults = [[1], [1], [1], [1], [1]]\n",
    "col1, col2, col3, col4, col5 = tf.decode_csv(\n",
    "    value, record_defaults=record_defaults)\n",
    "features = tf.stack([col1, col2, col3, col4])\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Start populating the filename queue.\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for i in range(12):\n",
    "        # Retrieve a single instance:\n",
    "        example, label = sess.run([features, col5])\n",
    "        print(example, label)\n",
    "\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write and read TFRecord data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def convert_to_tfrecord(data_set, filename):\n",
    "    images = data_set.images\n",
    "    labels = data_set.labels\n",
    "    num_examples = data_set.num_examples\n",
    "\n",
    "    if images.shape[0] != num_examples:\n",
    "        raise ValueError('Images size %d does not match label size %d.' %\n",
    "                         (images.shape[0], num_examples))\n",
    "    rows = images.shape[1]\n",
    "    cols = images.shape[2]\n",
    "\n",
    "    filename = os.path.join(FLAGS.directory, filename + '.tfrecords')\n",
    "    print('Writing', filename)\n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    for index in range(num_examples):\n",
    "        image_raw = images[index].tostring()\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'height': _int64_feature(rows),\n",
    "            'width': _int64_feature(cols),\n",
    "            'label': _int64_feature(int(labels[index])),\n",
    "            'image_raw': _bytes_feature(image_raw)}))\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "\n",
    "def read_and_decode(filename):\n",
    "    filename = os.path.join(FLAGS.directory, filename + '.tfrecords')\n",
    "    print('Reading', filename)\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        [filename], num_epochs=1)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)  # tf.TFRecordReader().read() only accept queue as param\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        # Defaults are not specified since both keys are required.\n",
    "        features={\n",
    "            'height': tf.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.FixedLenFeature([], tf.int64),\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "        })\n",
    "    image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "    height = tf.cast(features['height'], tf.int32)\n",
    "    width = tf.cast(features['width'], tf.int32)\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    image.set_shape([28 * 28])\n",
    "    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\n",
    "\n",
    "    return image, label\n",
    "\n",
    "def main(unused_argv):\n",
    "    '''\n",
    "    Generate TFRecord file\n",
    "    data_set = mnist.read_data_sets(FLAGS.directory,\n",
    "                                    dtype=tf.uint8,\n",
    "                                    reshape=False,\n",
    "                                    validation_size=FLAGS.validation_size)\n",
    "    convert_to_tfrecord(data_set.train, 'mnist_train')\n",
    "    convert_to_tfrecord(data_set.validation, 'mnist_validation')\n",
    "    convert_to_tfrecord(data_set.test, 'mnist_test')\n",
    "    '''\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        first_image, first_label = read_and_decode('mnist_train')\n",
    "\n",
    "        init_op = tf.group(tf.global_variables_initializer(),\n",
    "                           tf.local_variables_initializer())\n",
    "\n",
    "        sess = tf.Session()\n",
    "        # Run the initialization\n",
    "        sess.run(init_op)\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        image, label = sess.run([first_image, first_label])\n",
    "        print(label)\n",
    "        print(image)\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--directory',\n",
    "      type=str,\n",
    "      default='C:\\\\Boyuan\\\\MyPython\\\\MNIST_Dataset',\n",
    "      help='Directory to download data files and write the converted result'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--validation_size',\n",
    "      type=int,\n",
    "      default=5000,\n",
    "      help=\"\"\"\\\n",
    "      Number of examples to separate from the training data for the validation\n",
    "      set.\\\n",
    "      \"\"\"\n",
    "  )\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
